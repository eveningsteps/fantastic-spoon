\documentclass[a4paper, 12pt]{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,bindingoffset=0cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{amssymb, latexsym, amsmath}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{citehack}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{tikz}
\usepackage{pgfplots}

\lstloadlanguages{C++}
\lstset{extendedchars=false,
	breaklines=true,
	breakatwhitespace=true,
	keepspaces = true,
	tabsize=4
}

\begin{document}
\include{./title}
\newpage


\subsection*{Постановка задачи}
Необходимо разработать программу, рассчитывающую обратный индекс для всех слов по трём исходным текстам. Также необходимо подсчитать количество токенов, терминов, их средние длины и вывести время работы программы.


\subsection*{Алгоритм}
В качестве исходных текстов будут использоваться два романа и работа <<Исследование догматического богословия>> авторства Алексея Толстого, взятые с сайта lib.ru. Разобьем их на абзацы, заключённые в тэги {\tt <dd>}, и будем считать каждый абзац отдельным документом. Разобьём каждый документ на токены, которые превратим в термины (приведение к единому регистру, удаление знаков препинания); попутно запомним позицию терма в документе. Таким образом, получим словарь терминов, где для каждого термина записаны номера документов, в которых он встречается, и позиции в документе, то есть обратный индекс.


\subsection*{Реализация}
Программа написана на языке D; обратный индекс хранится в хэшмапе, ключом служит строка, значением --- хэшмап, где по номеру документа доступен список позиций вхождений термина.


\subsubsection*{Обработка текста}
\lstinputlisting{../src/indexer.d}


\subsection*{Результат выполнения}
\lstinputlisting{./result.txt}

\vspace{1cm}

\begin{tabular}{|c|c|c|}
\hline
Количество токенов & Количество терминов & Время работы, мс \\
\hline
277343 & 33634 & 530 \\
88030 & 14517 & 220 \\
148211 & 28939 & 344 \\
\hline
\end{tabular}

\vspace{1cm}

\begin{tikzpicture}[domain=0:3]
\begin{axis}[xlabel={Кол-во токенов}, ylabel={Время работы, мс}]
\addplot coordinates {
(  88030, 220 )
(148211, 344 )
(277343, 530 )
};
\end{axis}
\end{tikzpicture}

\subsection*{Выводы}
Средняя длина термина более чем в полтора раза больше средней длины токена, что говорит о том, что в тексте часто встречались короткие токены. Как станет известно поздней, это подтверждается и законом Ципфа.
\end{document}
